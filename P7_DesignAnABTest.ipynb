{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Free Trial Screener Experiment\n",
    "\n",
    "- [Description](#Description)\n",
    "- [Design](#Design)\n",
    "    * [Metric Choice](#Metric-Choice)\n",
    "        - [Invariant Metrics](#Invariant-Metrics)\n",
    "        - [Evaluation Metrics](#Evaluation-Metrics)\n",
    "        - [Unused Metrics](#Unused-Metrics)\n",
    "    * [Measuring Variability](#Measuring-Variability)\n",
    "    * [Sizing](#Sizing)\n",
    "        - [Number of Samples vs. Power](#Number-of-Samples-vs.-Power)\n",
    "        - [Duration vs. Exposure](#Duration-vs.-Exposure)\n",
    "- [Analysis](#Analysis)\n",
    "    * [Sanity Checks](#Sanity-Checks)\n",
    "    * [Result Analysis](#Result-Analysis)\n",
    "        - [Effect Size Tests](#Effect-Size-Tests)\n",
    "        - [Sign Tests](#Sign-Tests)\n",
    "    * [Summary](#Summary)\n",
    "    * [Recommendation](#Recommendation)\n",
    "- [Follow-up Experiment](#Follow-up-Experiment)\n",
    "- [References](#References)\n",
    "        \n",
    "        \n",
    "\n",
    "## Description\n",
    "\n",
    "At the time of this experiment, Udacity courses currently have\n",
    "two options on the home page: \"start free trial\", and \"access\n",
    "course materials\". If the student clicks \"start free trial\", they\n",
    "will be asked to enter their credit card information, and then\n",
    "they will be enrolled in a free trial for the paid version of the\n",
    "course. After 14 days, they will automatically be charged unless\n",
    "they cancel first. If the student clicks \"access course materials\",\n",
    "they will be able to view the videos and take the quizzes for free,\n",
    "but they will not receive coaching support or a verified certificate,\n",
    "and they will not submit their final project for feedback.\n",
    "\n",
    "In the experiment, Udacity tested a change where if the student\n",
    "clicked \"start free trial\", they were asked how much time they had\n",
    "available to devote to the course. If the student indicated 5 or\n",
    "more hours per week, they would be taken through the checkout\n",
    "process as usual. If they indicated fewer than 5 hours per week,\n",
    "a message would appear indicating that Udacity courses usually\n",
    "require a greater time commitment for successful completion, and\n",
    "suggesting that the student might like to access the course materials\n",
    "for free. At this point, the student would have the option to\n",
    "continue enrolling in the free trial, or access the course materials\n",
    "for free instead.\n",
    "\n",
    "This screenshot shows what the experiment looks like:  \n",
    "![](http://i.imgur.com/loVIMQw.png?1)\n",
    "\n",
    "The hypothesis was that this might set clearer expectations for\n",
    "students upfront, thus reducing the number of frustrated students\n",
    "who left the free trial because they didn't have enough timeâ€”without\n",
    "significantly reducing the number of students to continue past the\n",
    "free trial and eventually complete the course. If this hypothesis\n",
    "held true, Udacity could improve the overall student experience and\n",
    "improve coaches' capacity to support students who are likely to\n",
    "complete the course.\n",
    "\n",
    "The unit of diversion is a cookie, although if the student enrolls\n",
    "in the free trial, they are tracked by user-id from that point forward.\n",
    "The same user-id cannot enroll in the free trial twice. For users\n",
    "that do not enroll, their user-id is not tracked in the experiment,\n",
    "even if they were signed in when they visited the course overview page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "from util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design\n",
    "\n",
    "### Metric Choice\n",
    "There are two types of metrics that we have to choose.  \n",
    "One is **invariant metrics**, that is, metrics which the results are expected\n",
    "to be equal in the control and experiment group (or, the difference\n",
    "is not statistically significant).  \n",
    "Another one is **evaluation metrics**, that is, the metrics that\n",
    "are used to evaluate the experiment result, which should have\n",
    "statistically significant difference between control and experiment group.\n",
    "\n",
    "***Invariant Metrics***: Number of Cookies, Number of Clicks, Click-through-probability  \n",
    "***Evaluation Metrics***: Gross Conversion, Retention, Net Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invariant Metrics\n",
    "\n",
    "- **Number of Cookies**: Number of unique cookies to view the course overview page.  \n",
    "  The experiment will be *after* the user clicked \"Start free trial\" button,\n",
    "  number of page views shouldn't have difference between control and experiment group.\n",
    "- **Number of Clicks**: Number of unique cookies to click the \"Start free trial\" button\n",
    "  (which happens before the free trial screener is trigger).  \n",
    "  Again, the experiment will be *after* the user clicked, so it shouldn't have a difference.\n",
    "  Thus an appropriate invariant metric.\n",
    "- **Click-through-probability**: Number of unique cookies to click the \"Start free trial\"\n",
    "  button divided by number of unique cookies to view the course overview page.  \n",
    "  We're expecting both numerator and denominator to be invariant,\n",
    "  so the probability should also be invariant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "invariants = [\"num_cookies\", \"num_clicks\", \"click_thru_prob\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics\n",
    "\n",
    "- **Gross Conversion**: Number of user-ids to complete checkout\n",
    "  and enroll in the free trial divided by number of unique cookies to click\n",
    "  the \"Start free trial\" button.  \n",
    "  For this metric, we're expecting that there should be a *negative* change.\n",
    "  That is, the gross conversion of the experiment group should be less than\n",
    "  the control group, because the goal of the change is to prevent users from\n",
    "  enroll into the course without measuring his/her time.\n",
    "- **Retention**: Number of user-ids to remain enrolled past the 14-day boundary\n",
    "  (and thus make at least one payment) divided by number of user-ids to complete checkout.  \n",
    "  There should be an *increasing* change for retention, we want the enrollments\n",
    "  to be consistent, not just simply cancel it after 14 days free trial.\n",
    "- **Net Conversion**: number of user-ids to remain enrolled past the\n",
    "  14-day boundary (and thus make at least one payment) divided by the number\n",
    "  of unique cookies to click the \"Start free trial\" button.  \n",
    "  We're expecting to see a *non-decreasing* change (which is, remain or increase)\n",
    "  on net conversion. That is, it's not going to reduce the number of students\n",
    "  who stay beyond the free trial and make at least one payment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_metrics = [\"gross_conversion\", \"retention\", \"net_conversion\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unused Metrics\n",
    "- **Number of user-ids**: Number of users who enroll in the free trial.  \n",
    "  This isn't suitable for an invariant metric, because it supposed to change.\n",
    "  It could be an evaluation metric though, but I would rather use a normalised\n",
    "  fraction such as probabilities or rates for evaluation metrics. Thus, we\n",
    "  should use *Gross Conversion* instead of *Number of user-ids*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measuring Variability\n",
    "\n",
    "Here we're measuring variability of metrics *analytically*\n",
    "based on some predefined baseline values, which can be found in\n",
    "[this spreadsheet](https://docs.google.com/spreadsheets/d/1MYNUtC47Pg8hdoCjOXaHqF-thheGpUshrFA21BAJnNc/edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Baseline Conversion Rates\n",
    "COOKIES_PER_DAY = 40000\n",
    "CLICKS_PER_DAY  = 3200\n",
    "ENROLLS_PER_DAY = 660\n",
    "CLICK_THRU_PROB = 0.08\n",
    "P_ENROLL_CLICK  = 0.20625\n",
    "P_PAY_ENROLL    = 0.53\n",
    "P_PAY_CLICK     = 0.1093125\n",
    "\n",
    "# BCR, short for Baseline Conversion Rate\n",
    "metric_bcr = pd.Series([P_ENROLL_CLICK, P_PAY_ENROLL, P_PAY_CLICK], eval_metrics)\n",
    "\n",
    "unit_analysis = pd.Series([CLICK_THRU_PROB,\n",
    "                           P_ENROLL_CLICK * CLICK_THRU_PROB,\n",
    "                           CLICK_THRU_PROB],\n",
    "                         eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Standard Deviation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gross_conversion</th>\n",
       "      <td>0.020231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retention</th>\n",
       "      <td>0.054949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>net_conversion</th>\n",
       "      <td>0.015602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Standard Deviation\n",
       "gross_conversion            0.020231\n",
       "retention                   0.054949\n",
       "net_conversion              0.015602"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_SIZE = 5000\n",
    "\n",
    "metrics_sd = se_binom(metric_bcr, SAMPLE_SIZE * unit_analysis)\n",
    "pd.DataFrame({\"Standard Deviation\": metrics_sd})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analytic estimates are likely to be comparable to the empirical\n",
    "estimates if unit of analysis is equal to unit of diversion, which\n",
    "applies to *gross conversion* and *net conversion*. For *retention*,\n",
    "we might want to collect the empirical estimate if we have time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sizing\n",
    "\n",
    "#### Number of Samples vs. Power\n",
    "\n",
    "I won't be using Bonferroni correction for my analysis.  \n",
    "For the number of page views needed, we have to calculate the requirements\n",
    "for each evaluation metric, then use the maximum as our requirement for\n",
    "page views.  \n",
    "Here I'm using $\\alpha = 0.05, \\beta = 0.2$ to calculate the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pageviews Required</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gross_conversion</th>\n",
       "      <td>645875.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>net_conversion</th>\n",
       "      <td>685325.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retention</th>\n",
       "      <td>4741212.121212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Pageviews Required\n",
       "gross_conversion       645875.000000\n",
       "net_conversion         685325.000000\n",
       "retention             4741212.121212"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Practical significance levels\n",
    "d_min = pd.Series([0.01, 0.01, 0.0075], eval_metrics)\n",
    "\n",
    "pageviews_needed = 2 * required_samples(metric_bcr, d_min, alpha=0.05, beta=0.2) / unit_analysis\n",
    "pd.DataFrame({\"Pageviews Required\": pageviews_needed.sort_values()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That says, we need at least 4741213 page views to have enough power for *each* metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Duration vs. Exposure\n",
    "\n",
    "This experiment isn't quite risky since we're not expecting to see a descreasing\n",
    "change on number of enrollments. We can divert more than half of traffic to the\n",
    "experiment group, here I choose 80%. Given this, we can now estimate the\n",
    "duration of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Duration in Days</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gross_conversion</th>\n",
       "      <td>20.183594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retention</th>\n",
       "      <td>148.162879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>net_conversion</th>\n",
       "      <td>21.416406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Duration in Days\n",
       "gross_conversion         20.183594\n",
       "retention               148.162879\n",
       "net_conversion           21.416406"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAFFIC_FRACTION = 0.8\n",
    "\n",
    "duration = pageviews_needed / (COOKIES_PER_DAY * TRAFFIC_FRACTION)\n",
    "pd.DataFrame({\"Duration in Days\": duration})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result, if we need enough power for every metric, we'll need 149 days to\n",
    "run the experiment, which is a really long. In fact, even if we divert 100%\n",
    "of the traffic to the experiment group, we still need 119 days. We might want\n",
    "to eliminate retention from our evaluation metrics, so that the experiment\n",
    "won't take too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_metrics = [\"gross_conversion\", \"net_conversion\"]\n",
    "for series in [metric_bcr, unit_analysis, metrics_sd, d_min, pageviews_needed, duration]:\n",
    "    series.drop(\"retention\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we only use gross conversion and net conversion as our evaluation\n",
    "metrics, the minimum page views required drops down to 685325, and the duration\n",
    "also comes to 22, which is much more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "The experiment data can be found in [this spreadsheet](https://docs.google.com/spreadsheets/d/1Mu5u9GrybDdska-ljPXyBjTpdZIUev_6i7t4LRDfXM8/edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conts and exps are the summary of cont and exp\n",
    "cont, conts, exp, exps = get_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Checks\n",
    "\n",
    "For each invariant metric, we're going to calculate the 95% confidence\n",
    "interval for the value we expect to observe, then check if the observed\n",
    "value is actually between the lower and upper bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Expected</th>\n",
       "      <th>Lower Bound</th>\n",
       "      <th>Upper Bound</th>\n",
       "      <th>Observed</th>\n",
       "      <th>Pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>num_cookies</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.49882</td>\n",
       "      <td>0.50118</td>\n",
       "      <td>0.50064</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_clicks</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.495885</td>\n",
       "      <td>0.504115</td>\n",
       "      <td>0.500467</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>click_thru_prob</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.00129566</td>\n",
       "      <td>0.00129566</td>\n",
       "      <td>5.66271e-05</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Expected Lower Bound Upper Bound     Observed  Pass\n",
       "num_cookies          0.5     0.49882     0.50118      0.50064  True\n",
       "num_clicks           0.5    0.495885    0.504115     0.500467  True\n",
       "click_thru_prob        0 -0.00129566  0.00129566  5.66271e-05  True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sanity_checks(conts, exps, invariants,\n",
    "              metric_types=[\"sum\", \"sum\", \"prob\"],\n",
    "              units=[\"Pageviews\", \"Clicks\", (\"Clicks\", \"Pageviews\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cont, conts, exp, exps = cleanup_data(cont, exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effect Size Tests\n",
    "\n",
    "For each metric, I'm going to calculate the 95% confidence interval around\n",
    "the difference between experiment and control groups. Then check if 0 and\n",
    "the practical significance level, $d_{min}$ are between the interval. If\n",
    "they don't, we'll be confident that there's a difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lower Bound</th>\n",
       "      <th>Upper Bound</th>\n",
       "      <th>Statistical Significance</th>\n",
       "      <th>Practical Significance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gross_conversion</th>\n",
       "      <td>-0.029123</td>\n",
       "      <td>-0.011987</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>net_conversion</th>\n",
       "      <td>-0.011605</td>\n",
       "      <td>0.001857</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Lower Bound  Upper Bound Statistical Significance  \\\n",
       "gross_conversion    -0.029123    -0.011987                     True   \n",
       "net_conversion      -0.011605     0.001857                    False   \n",
       "\n",
       "                 Practical Significance  \n",
       "gross_conversion                   True  \n",
       "net_conversion                    False  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effect_size_tests(conts, exps, eval_metrics,\n",
    "                  unit_X=[\"Enrollments\", \"Payments\"],\n",
    "                  unit_N=[\"Clicks\",      \"Clicks\"],\n",
    "                  d_min=d_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sign Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p-value</th>\n",
       "      <th>Statistical Significance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gross_conversion</th>\n",
       "      <td>0.00259948</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>net_conversion</th>\n",
       "      <td>0.677639</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     p-value Statistical Significance\n",
       "gross_conversion  0.00259948                     True\n",
       "net_conversion      0.677639                    False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sign_tests(cont, exp, eval_metrics,\n",
    "           unit_X=[\"Enrollments\", \"Payments\"],\n",
    "           unit_N=[\"Clicks\",      \"Clicks\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In the experiment analysis, I didn't use Bonferroni correction, because it's\n",
    "not so useful for our case. Bonferroni correction is for reducing false\n",
    "positive results (Type I errors) when the criteria for launching the experiment\n",
    "is to have *any* of the metrics shown up positive, in that situation, if we have\n",
    "many evaluation metrics, there will be a higher probability that the change have\n",
    "significant result *just by chance*, thus deploy Bonferroni correction to reduce\n",
    "the chance of happening this. However, in our case, we want *all* the metrics to\n",
    "have significant result, so Bonferroni correction isn't so useful here.  \n",
    "The sanity checks were all passed, all the observed statistics are inside the\n",
    "95% confidence intervals for each invariant metrics' expected values.  \n",
    "For the experiment results, *gross conversion* resulted to be both statistically\n",
    "and practically significant. *Net conversion* in the other hand, shown up neither\n",
    "statistically nor practically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation\n",
    "\n",
    "Our experiment result shows that there's a significant change on *gross conversion*,\n",
    "which means that there are significantly less students enroll into free trial, and\n",
    "that's what we want to see.\n",
    "For *net conversion*, there's no significant difference, which is also what we want\n",
    "to see, it's not significantly reducing the students who continue pass the free trial;\n",
    "*However*, the confidence interval actually includes the negetive of the practical\n",
    "significance level, so it's possible that this number went down by an amount that\n",
    "would matter to the business, which is extremely important.\n",
    "Based on these, I would recommend to *not* launch the change, or run an another\n",
    "experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow-up Experiment\n",
    "\n",
    "For myself, I think I sometimes over or under estimate the time I'm going to need\n",
    "for a course. It's probably a good idea to have a warm-up course or project that is\n",
    "simple enough to complete in 5 hours and also covers prerequisite knowledges, so if\n",
    "the student can complete it within a week, then he'll have a more practical sense\n",
    "of the time commitment.\n",
    "\n",
    "#### Hypothesis\n",
    "\n",
    "The hypothesis for this experiment is that students will have more practical sense\n",
    "of the time commitment, thus reducing early cancellations due to the lack of time.\n",
    "\n",
    "#### Unit of Diversion\n",
    "\n",
    "The unit of diversion will be the user-id, because the experiment is *after* the\n",
    "student hits the \"Start free trial\" button.\n",
    "\n",
    "#### Invariant Metrics\n",
    "\n",
    "The invariant metrics for this experiment will be **Number of Cookies**, **Number\n",
    "of User-ids**, **Number of Clicks** and **Click-through-probability**. The three\n",
    "metrics we've already used in the previous experiment are having the same reasons as\n",
    "in the previous experiment. For the *Number of User-ids*, because this experiment\n",
    "begins *after* users enrolled into free trial, so the number of users who enrolled\n",
    "should be invariant.\n",
    "\n",
    "#### Evaluation Metrics\n",
    "\n",
    "I'll use one of the metrics we had in the previous experiment, **Retention**, as the\n",
    "evaluation metric in this new experiment since we want to keep students beyond free\n",
    "trial. Retention measures how likely the students are going to stay beyond free trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[https://www.udacity.com/course/ab-testing--ud257](https://www.udacity.com/course/ab-testing--ud257)  \n",
    "[https://en.wikipedia.org/wiki/Multiple_comparisons_problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem)  \n",
    "[https://docs.google.com/document/u/1/d/1aCquhIqsUApgsxQ8-SQBAigFDcfWVVohLEXcV6jWbdI/pub?embedded=True](https://docs.google.com/document/u/1/d/1aCquhIqsUApgsxQ8-SQBAigFDcfWVVohLEXcV6jWbdI/pub?embedded=True)\n",
    "[https://review.udacity.com/#!/reviews/166251/shared](https://review.udacity.com/#!/reviews/166251/shared)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
